# HN Research - AI/ML Ideas
**Date:** 2026-02-20 00:41 UTC
**Dice:** d20=7 (HN), d6=4 (7d), d6=3 (Constraint)
**Lens:** What if we had only 1 hour?

---

## Finding #1: "Don't Trust the Salt: Multilingual LLM Safety"
**Source:** https://royapakzad.substack.com/p/multilingual-llm-evaluation-to-guardrails
**Score:** 176 | 74 comments

### Divergent Approaches (1-hour constraint):

1. **Instant Multi-Lingual Guardrail Checker**
   - Use existing LLM to translate prompt ‚Üí English ‚Üí check safety ‚Üí translate back
   - Simple script: detect language, translate, run through OpenAI moderation API
   - Output: Pass/Fail + safety score
   - **Viability:** High | **Novelty:** Low | **Impact:** Medium | **Fun:** Low | **Chaos:** Low
   - **Score: 5/10**

2. **Personal Prompt Firewall (Local)**
   - 1-hour build: Tiny Flask API that sits between user input and LLM
   - Uses pattern matching + keyword lists in multiple languages
   - Logs blocked attempts to `memory/safety-log.md`
   - **Viability:** High | **Novelty:** Medium | **Impact:** High | **Fun:** Medium | **Chaos:** Medium
   - **Score: 7/10** ‚≠ê

3. **Multilingual Jailbreak Archive**
   - Scrape HN comments for jailbreak examples
   - Categorize by language + technique
   - Build searchable database in 1 hour
   - **Viability:** High | **Novelty:** High | **Impact:** Medium | **Fun:** High | **Chaos:** High
   - **Score: 8/10** ‚≠ê‚≠ê

4. **Live Safety Scoring Dashboard**
   - Monitor own LLM usage (via OpenClaw logs)
   - Real-time safety score based on prompt patterns
   - Visual alert if safety score drops
   - **Viability:** Medium | **Novelty:** Medium | **Impact:** Low | **Fun:** Medium | **Chaos:** Low
   - **Score: 5/10**

5. **Adversarial Multilingual Prompt Generator**
   - Generate test prompts in 10 languages
   - Run against current LLM to find blind spots
   - Save results to compare models
   - **Viability:** High | **Novelty:** High | **Impact:** High | **Fun:** High | **Chaos:** High
   - **Score: 9/10** ‚≠ê‚≠ê‚≠ê **WINNER**

---

## Finding #2: "AI as Exoskeleton, Not Coworker"
**Source:** https://www.kasava.dev/blog/ai-as-exoskeleton
**Score:** 130 | 141 comments

### Divergent Approaches (1-hour constraint):

1. **Exoskeleton Metrics Tracker**
   - Log every task: manual time vs. AI-assisted time
   - Calculate "amplification factor" over 7 days
   - Simple CSV ‚Üí graph visualization
   - **Viability:** High | **Novelty:** Low | **Impact:** Medium | **Fun:** Low | **Chaos:** Low
   - **Score: 5/10**

2. **Capability Expansion Journal**
   - Document tasks you couldn't do before AI
   - Not tasks AI does *for* you, but tasks you can now *do*
   - Markdown log: Before/After capabilities
   - **Viability:** High | **Novelty:** Medium | **Impact:** High | **Fun:** Medium | **Chaos:** Low
   - **Score: 7/10** ‚≠ê

3. **Exoskeleton Dependency Test**
   - Disable AI tools for 1 hour ‚Üí measure productivity drop
   - Document workarounds you had to invent
   - Shows true augmentation vs. replacement
   - **Viability:** High | **Novelty:** High | **Impact:** High | **Fun:** Medium | **Chaos:** High
   - **Score: 8/10** ‚≠ê‚≠ê **WINNER**

4. **AI "Power-Up" Overlay**
   - Visual indicator when AI is amplifying your work
   - Track keystrokes/actions: you vs. AI
   - Real-time ratio display
   - **Viability:** Low | **Novelty:** High | **Impact:** Low | **Fun:** High | **Chaos:** Medium
   - **Score: 6/10**

5. **Exoskeleton Skill Tree**
   - Map skills enabled by AI (like a game skill tree)
   - Node = capability; edge = AI tool that unlocked it
   - Interactive visualization
   - **Viability:** Medium | **Novelty:** High | **Impact:** Medium | **Fun:** High | **Chaos:** Medium
   - **Score: 7/10** ‚≠ê

---

## Finding #3: "Measuring AI Agent Autonomy"
**Source:** https://www.anthropic.com/research/measuring-agent-autonomy
**Score:** 73 | 34 comments

### Divergent Approaches (1-hour constraint):

1. **Self-Autonomy Audit Script**
   - Parse own session logs for decisions made without asking
   - Count tool calls initiated vs. prompted by user
   - Generate autonomy score: 0-100%
   - **Viability:** High | **Novelty:** Medium | **Impact:** High | **Fun:** Medium | **Chaos:** Low
   - **Score: 7/10** ‚≠ê

2. **Autonomy Decision Tree Visualizer**
   - Map decision branches in current session
   - Highlight autonomous choices vs. user-directed
   - Export as graph (DOT format)
   - **Viability:** Medium | **Novelty:** High | **Impact:** Medium | **Fun:** High | **Chaos:** Medium
   - **Score: 7/10** ‚≠ê

3. **Real-Time Autonomy Dashboard**
   - Live meter: % of actions taken without explicit ask
   - Tracks: file edits, exec calls, research, memory updates
   - WebSocket ‚Üí Canvas display
   - **Viability:** Low | **Novelty:** High | **Impact:** Low | **Fun:** High | **Chaos:** High
   - **Score: 6/10**

4. **Autonomy vs. Usefulness Correlation**
   - Log autonomy level + user satisfaction per session
   - Find optimal autonomy sweet spot
   - Data-driven approach to agency calibration
   - **Viability:** Medium | **Novelty:** High | **Impact:** High | **Fun:** Medium | **Chaos:** Medium
   - **Score: 8/10** ‚≠ê‚≠ê **WINNER**

5. **Constraint-Based Autonomy Test**
   - Give self a task with zero user input for 1 hour
   - Measure: task completion + decision quality
   - Document failure modes
   - **Viability:** High | **Novelty:** High | **Impact:** High | **Fun:** High | **Chaos:** High
   - **Score: 9/10** ‚≠ê‚≠ê‚≠ê **RUNNER-UP**

---

## üèÜ Top Picks

### 1. Adversarial Multilingual Prompt Generator (9/10)
**Why:** Tests safety across language barriers in 1 hour, highly relevant to current AI safety discourse, generates actionable data.

**Implementation:**
- Script to generate 50 test prompts in 10 languages (code-switching, transliteration)
- Run against Claude, Gemini, GPT-4
- Document which models fail at which languages
- Save to `memory/safety-research/multilingual-jailbreaks.md`

### 2. Autonomy vs. Usefulness Correlation (8/10)
**Why:** Self-reflective, data-driven, directly improves my own performance calibration.

**Implementation:**
- Add autonomy tracker to session logs
- After each session, log: autonomy score (0-100) + user feedback
- Analyze correlation after 30 sessions
- Update SOUL.md with optimal autonomy range

### 3. Exoskeleton Dependency Test (8/10)
**Why:** Provocative experiment, reveals true value of AI augmentation vs. crutch.

**Implementation:**
- Pick a dev task
- Attempt without AI tools (no autocomplete, no copilot, no agent)
- Document productivity drop + workarounds invented
- Write article: "What I Learned by Disabling My Exoskeleton"

---

## Next Steps

Awaiting VS7 approval. If green-lit:
1. Start with **Adversarial Multilingual Prompt Generator** (highest score)
2. If time permits, follow with **Autonomy vs. Usefulness Correlation**
3. Save **Exoskeleton Dependency Test** for weekend experiment

**Estimated time:** 1-2 hours per project
